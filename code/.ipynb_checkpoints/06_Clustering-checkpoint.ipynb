{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check (That Tweet) Yo Self \n",
    "## Prioritizing Tweets to Fact Check\n",
    "###### Part 6: Clustering (Unsupervised Learning)\n",
    "In the previous notebooks, we collected tweets about Coronavirus from around the time of Donald Trump's lysol comment and connected them with a separate pull on user data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gathered all the neccesary user information, we are going to cluster based on user stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('../data/tweet_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweet[:33200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll engineer a few features that takes a closer look at the tweet text and user info. Descriptions for each features are below:\n",
    "- **'len_user'** : lenth of the username\n",
    "- **'big_feelings'** : percentage of the tweet in uppercase\n",
    "- **'ratio'** : user's followers divded by how many they are following\n",
    "- **'has_url'** : user has a url as part of their profile\n",
    "- **'has_location'** : user has a location listed on their profile\n",
    "- **'has_bio'** : user has a bio as part of their profile\n",
    "- **'len_bio'** : length of user's bio\n",
    "- **'ratio_num_user'** : percentage of numeric characters in the username\n",
    "- **'emotional_range'** : idea stemmed from this [paper](https://hpi.de/fileadmin/user_upload/fachgebiete/naumann/publications/2013/Analyzing_and_Predicting_Viral_Tweets.pdf), it's the absoluate value of postive and negative emotion added together and divided by 10 to explain the range of emotions used in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['len_user'] = [len(x) for x in tweet['author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_upper(string):\n",
    "    count = 0\n",
    "    for s in string:\n",
    "         if s == s.upper():\n",
    "                count += 1\n",
    "    ratio = count / len(string)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['big_feelings'] = tweet['text'].apply(per_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(followers, following):\n",
    "    if following == 0:\n",
    "        following = 1\n",
    "    elif followers == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return followers / following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['ratio'] = [get_ratio(m, n) for m, n in zip(tweet['user_followers'], tweet['user_following'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['has_url'] = tweet['user_url'].notna().astype(int)\n",
    "tweet['has_location'] = tweet['user_location'].notna().astype(int)\n",
    "tweet['has_bio'] = tweet['user_bio'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['len_bio'] = [len(str(x)) for x in tweet['user_bio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(string):\n",
    "    count = 0\n",
    "    for s in string:\n",
    "        try:\n",
    "            int(s)\n",
    "            count += 1\n",
    "        except:\n",
    "            count = count\n",
    "    return count / len(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['ratio_num_user'] = tweet['author'].apply(is_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_range(string):\n",
    "    emo_r = np.abs(sent.polarity_scores(string)['neg']) + sent.polarity_scores(string)['pos']\n",
    "    return emo_r / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['emotional_range'] = tweet['text'].apply(emotion_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.drop(columns = ['user_bio', 'user_location', 'user_url'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From earlier investigation of specific accounts, we concluded if a person has \"null\" favorites, it is because they have not favorited anything so we can impute \"0\" for the null values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['user_favorites'] = tweet['user_favorites'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise for ratio, the values showing up as null are when they have 0 followers and 0 following. While this might be a ratio of 1 (equal followers/following), this really shows a lack of connection and engagement so it would be more appropriate to fill in 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['ratio'] = tweet['ratio'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 33199 entries, 0 to 33199\n",
      "Data columns (total 37 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   id                  33199 non-null  int64  \n",
      " 1   time                33199 non-null  object \n",
      " 2   author              33199 non-null  object \n",
      " 3   author_id           33199 non-null  int64  \n",
      " 4   associated_tweet    33199 non-null  int64  \n",
      " 5   text                33199 non-null  object \n",
      " 6   links               33199 non-null  object \n",
      " 7   hashtags            33199 non-null  object \n",
      " 8   mentions            33199 non-null  object \n",
      " 9   reply_count         33199 non-null  int64  \n",
      " 10  favorite_count      33199 non-null  int64  \n",
      " 11  retweet_count       33199 non-null  int64  \n",
      " 12  day                 33199 non-null  object \n",
      " 13  not_english         33199 non-null  float64\n",
      " 14  hashtag_count       33199 non-null  int64  \n",
      " 15  mention_count       33199 non-null  int64  \n",
      " 16  word_count          33199 non-null  int64  \n",
      " 17  char_count          33199 non-null  int64  \n",
      " 18  link_count          33199 non-null  int64  \n",
      " 19  text_sentiment      33199 non-null  float64\n",
      " 20  text_links_removed  33199 non-null  object \n",
      " 21  clean_text          33199 non-null  object \n",
      " 22  clean_word_count    33199 non-null  int64  \n",
      " 23  clean_char_count    33199 non-null  int64  \n",
      " 24  user_tweets         33199 non-null  int64  \n",
      " 25  user_following      33199 non-null  int64  \n",
      " 26  user_followers      33199 non-null  int64  \n",
      " 27  user_favorites      33199 non-null  float64\n",
      " 28  len_user            33199 non-null  int64  \n",
      " 29  big_feelings        33199 non-null  float64\n",
      " 30  ratio               33199 non-null  float64\n",
      " 31  has_url             33199 non-null  int64  \n",
      " 32  has_location        33199 non-null  int64  \n",
      " 33  has_bio             33199 non-null  int64  \n",
      " 34  len_bio             33199 non-null  int64  \n",
      " 35  ratio_num_user      33199 non-null  float64\n",
      " 36  emotional_range     33199 non-null  float64\n",
      "dtypes: float64(7), int64(21), object(9)\n",
      "memory usage: 9.6+ MB\n"
     ]
    }
   ],
   "source": [
    "tweet.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have no missing values and can continue to clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the user attributes we want to focus our clustering on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cat = ['user_tweets', 'user_following', 'user_followers', \n",
    "             'ratio', 'has_url', 'has_location', 'has_bio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cluster = tweet[user_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data before creating clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cluster = ss.transform(to_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After try a range of different values for number of clusters, 5 gave the highest jump in silhouette score while still having distinct characteristics between the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=10, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=21, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=10, random_state = 21)\n",
    "km.fit(z_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps = .8, min_samples = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='auto', eps=0.8, leaf_size=30, metric='euclidean',\n",
       "       metric_params=None, min_samples=1000, n_jobs=None, p=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.fit(z_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7495098100757066"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(z_cluster, km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7675174323581028"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(z_cluster, db.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DBSCAN score is slightly higher and is producing more logical groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 2, 0, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling this model to use later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(db, open('../models/dbscan.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ss, open('../models/standardscaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['user_group_db'] = db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the distribution of our clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    16418\n",
       " 3     7342\n",
       " 1     4247\n",
       "-1     1903\n",
       " 4     1768\n",
       " 2     1521\n",
       "Name: user_group_db, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['user_group_db'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding these cluster groups to the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our clusters didn't take into account the specific \"reach\" (reply, favorite, and retweet count) for the tweet, we want to see how different clusters measure up in this capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['target'] = tweet['reply_count'] + tweet['favorite_count'] + tweet['retweet_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>associated_tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>links</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>...</th>\n",
       "      <th>big_feelings</th>\n",
       "      <th>ratio</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_location</th>\n",
       "      <th>has_bio</th>\n",
       "      <th>len_bio</th>\n",
       "      <th>ratio_num_user</th>\n",
       "      <th>emotional_range</th>\n",
       "      <th>user_group_db</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1254190074595553281</td>\n",
       "      <td>2020-04-25 16:26:30</td>\n",
       "      <td>Iam_helenna</td>\n",
       "      <td>215204985</td>\n",
       "      <td>1254190074595553281</td>\n",
       "      <td>Today, we have 1182 cases in Nigeria with 35 d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>1.357576</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1253828209075990531</td>\n",
       "      <td>2020-04-24 16:28:34</td>\n",
       "      <td>KerryeHill</td>\n",
       "      <td>2807727004</td>\n",
       "      <td>1253697753479331840</td>\n",
       "      <td>There's no such thing as a medical disinfectan...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217742</td>\n",
       "      <td>0.241706</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                 time       author   author_id  \\\n",
       "0  1254190074595553281  2020-04-25 16:26:30  Iam_helenna   215204985   \n",
       "1  1253828209075990531  2020-04-24 16:28:34   KerryeHill  2807727004   \n",
       "\n",
       "      associated_tweet                                               text  \\\n",
       "0  1254190074595553281  Today, we have 1182 cases in Nigeria with 35 d...   \n",
       "1  1253697753479331840  There's no such thing as a medical disinfectan...   \n",
       "\n",
       "  links hashtags mentions  reply_count  ...  big_feelings     ratio has_url  \\\n",
       "0    []     ['']     ['']           37  ...      0.255556  1.357576       0   \n",
       "1    []     ['']     ['']            1  ...      0.217742  0.241706       0   \n",
       "\n",
       "   has_location  has_bio  len_bio  ratio_num_user  emotional_range  \\\n",
       "0             0        1      147             0.0           0.0195   \n",
       "1             0        1       89             0.0           0.0140   \n",
       "\n",
       "   user_group_db  target  \n",
       "0              0     289  \n",
       "1              0       3  \n",
       "\n",
       "[2 rows x 39 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving this DataFrame with new features and cluster assignments to a csv for further EDA in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.to_csv('../data/user_cluster_tweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33199, 39)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realized later on that DBSCAN doesn't make prediction so we would have to train a classifer on the labels in order to analyze new information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn = z_cluster\n",
    "y_knn = tweet['user_group_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xk_train, Xk_test, yk_train, yk_test = train_test_split(X_knn, y_knn, random_state = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(Xk_train, yk_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993574039118037"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(Xk_train, yk_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985542168674699"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(Xk_test, yk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling this model for use later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(knn, open('../models/knn.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
