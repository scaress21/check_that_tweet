{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check (That Tweet) Yo Self \n",
    "## Prioritizing Tweets to Fact Check\n",
    "###### Part 9: Cluster NLP EDA\n",
    "This notebook looks at the top words and phrases for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import re\n",
    "import statistics\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(824)\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# Import stopwords.\n",
    "from nltk.corpus import stopwords # Import the stopword list\n",
    "import nltk\n",
    "\n",
    "from tweetscrape.users_scrape import TweetScrapperUser\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('../data/user_cluster_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>associated_tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>links</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>...</th>\n",
       "      <th>ratio</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_location</th>\n",
       "      <th>has_bio</th>\n",
       "      <th>len_bio</th>\n",
       "      <th>ratio_num_user</th>\n",
       "      <th>emotional_range</th>\n",
       "      <th>user_group_db</th>\n",
       "      <th>user_group</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1254190074595553281</td>\n",
       "      <td>2020-04-25 16:26:30</td>\n",
       "      <td>Iam_helenna</td>\n",
       "      <td>215204985</td>\n",
       "      <td>1254190074595553281</td>\n",
       "      <td>Today, we have 1182 cases in Nigeria with 35 d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>1.357576</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1253828209075990531</td>\n",
       "      <td>2020-04-24 16:28:34</td>\n",
       "      <td>KerryeHill</td>\n",
       "      <td>2807727004</td>\n",
       "      <td>1253697753479331840</td>\n",
       "      <td>There's no such thing as a medical disinfectan...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241706</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1253460644294283265</td>\n",
       "      <td>2020-04-23 16:08:00</td>\n",
       "      <td>Lmt48430438</td>\n",
       "      <td>1232381432988930049</td>\n",
       "      <td>1253460644294283265</td>\n",
       "      <td>Waiting to see how many people drink disinfect...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>['@DarcysCartoon']</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1254194987945865217</td>\n",
       "      <td>2020-04-25 16:46:01</td>\n",
       "      <td>iamshollyyoung</td>\n",
       "      <td>3096323025</td>\n",
       "      <td>1254194987945865217</td>\n",
       "      <td>Today I know there's no result Nigeria can not...</td>\n",
       "      <td>['https://t.co/RRuHGBH1SI']</td>\n",
       "      <td>['#Covid_19']</td>\n",
       "      <td>['']</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470899</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1253835841685934081</td>\n",
       "      <td>2020-04-24 16:58:54</td>\n",
       "      <td>toddcusuman</td>\n",
       "      <td>588727638</td>\n",
       "      <td>1253835841685934081</td>\n",
       "      <td>New York rapper Fred the Godson dies at 35 aft...</td>\n",
       "      <td>['https://t.co/rXOi5YEoZl']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                 time          author  \\\n",
       "0  1254190074595553281  2020-04-25 16:26:30     Iam_helenna   \n",
       "1  1253828209075990531  2020-04-24 16:28:34      KerryeHill   \n",
       "2  1253460644294283265  2020-04-23 16:08:00     Lmt48430438   \n",
       "3  1254194987945865217  2020-04-25 16:46:01  iamshollyyoung   \n",
       "4  1253835841685934081  2020-04-24 16:58:54     toddcusuman   \n",
       "\n",
       "             author_id     associated_tweet  \\\n",
       "0            215204985  1254190074595553281   \n",
       "1           2807727004  1253697753479331840   \n",
       "2  1232381432988930049  1253460644294283265   \n",
       "3           3096323025  1254194987945865217   \n",
       "4            588727638  1253835841685934081   \n",
       "\n",
       "                                                text  \\\n",
       "0  Today, we have 1182 cases in Nigeria with 35 d...   \n",
       "1  There's no such thing as a medical disinfectan...   \n",
       "2  Waiting to see how many people drink disinfect...   \n",
       "3  Today I know there's no result Nigeria can not...   \n",
       "4  New York rapper Fred the Godson dies at 35 aft...   \n",
       "\n",
       "                         links       hashtags            mentions  \\\n",
       "0                           []           ['']                ['']   \n",
       "1                           []           ['']                ['']   \n",
       "2                           []           ['']  ['@DarcysCartoon']   \n",
       "3  ['https://t.co/RRuHGBH1SI']  ['#Covid_19']                ['']   \n",
       "4  ['https://t.co/rXOi5YEoZl']           ['']                ['']   \n",
       "\n",
       "   reply_count  ...     ratio  has_url has_location  has_bio  len_bio  \\\n",
       "0           37  ...  1.357576        0            0        1      147   \n",
       "1            1  ...  0.241706        0            0        1       89   \n",
       "2            1  ...  0.357143        0            0        0        3   \n",
       "3            1  ...  0.470899        1            1        1      103   \n",
       "4            0  ...  0.077656        0            0        0        3   \n",
       "\n",
       "   ratio_num_user  emotional_range  user_group_db  user_group  target  \n",
       "0        0.000000           0.0195              0           0     289  \n",
       "1        0.000000           0.0140              0           0       3  \n",
       "2        0.727273           0.0000              1           2       1  \n",
       "3        0.000000           0.0216              2           4      55  \n",
       "4        0.000000           0.0000              1           2       0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional cleaning to the clean tweets column with custom stopwords that are significant in number but hold no contextual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn's stopwords, extracted\n",
    "sklearn_stopwords = list(CountVectorizer(stop_words = 'english').get_stop_words())\n",
    "#Custom created list\n",
    "custom_stopwords = ['com',\n",
    "                    'twitter',\n",
    "                    'pic',\n",
    "                    'http',\n",
    "                    'isolation',\n",
    "                    'pandemic',\n",
    "                    'covid',\n",
    "                    'quarantine',\n",
    "                    'vaccine',\n",
    "                    'coronavirus',\n",
    "                    'lysol',\n",
    "                    'ingest',\n",
    "                    'inject',\n",
    "                    'disinfectant',\n",
    "                    'bleach',\n",
    "                    'don',\n",
    "                    've']\n",
    "# Personalized stopwords\n",
    "personal_stopwords = sklearn_stopwords + custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_words(raw_tweet):\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    tweet_text = BeautifulSoup(raw_tweet).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", tweet_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. Convert personalized stopwords to set\n",
    "    stops = set(personal_stopwords)\n",
    "\n",
    "    # 5. Remove stopwords.\n",
    "    meaningful_words = [w for w in words if w not in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33199 tweets related to coronavirus.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of tweets based on the dataframe size.\n",
    "total_tweets = tweet.shape[0]\n",
    "print(f'There are {total_tweets} tweets related to coronavirus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing twitter data...\n",
      "Tweet 5000 of 33199.\n",
      "Tweet 10000 of 33199.\n",
      "Tweet 15000 of 33199.\n",
      "Tweet 20000 of 33199.\n",
      "Tweet 25000 of 33199.\n",
      "Tweet 30000 of 33199.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean tweets.\n",
    "clean_tweets = []\n",
    "\n",
    "print(\"Cleaning and parsing twitter data...\")\n",
    "\n",
    "# Instantiate counter.\n",
    "j = 0\n",
    "\n",
    "# For every post in our training set...\n",
    "for string in tweet['text_links_removed']:\n",
    "    \n",
    "    # Convert post to words, then append to clean_train_posts.\n",
    "    clean_tweets.append(tweet_to_words(string))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message.\n",
    "    if (j + 1) % 5000 == 0:\n",
    "        print(f'Tweet {j + 1} of {total_tweets}.')\n",
    "    \n",
    "    j += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweet.assign(clean_text = clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>associated_tweet</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>not_english</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>big_feelings</th>\n",
       "      <th>ratio</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_location</th>\n",
       "      <th>has_bio</th>\n",
       "      <th>len_bio</th>\n",
       "      <th>ratio_num_user</th>\n",
       "      <th>emotional_range</th>\n",
       "      <th>user_group</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_group_db</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.253942e+18</td>\n",
       "      <td>4.277929e+17</td>\n",
       "      <td>1.253449e+18</td>\n",
       "      <td>0.380268</td>\n",
       "      <td>2.052272</td>\n",
       "      <td>0.273605</td>\n",
       "      <td>0.260551</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.145750</td>\n",
       "      <td>25.222745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>1.127193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.139479</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.706146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.253957e+18</td>\n",
       "      <td>5.748085e+17</td>\n",
       "      <td>1.253366e+18</td>\n",
       "      <td>0.904011</td>\n",
       "      <td>5.859303</td>\n",
       "      <td>1.370809</td>\n",
       "      <td>0.278667</td>\n",
       "      <td>0.402367</td>\n",
       "      <td>0.202498</td>\n",
       "      <td>26.662722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267239</td>\n",
       "      <td>2.813750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>48.174227</td>\n",
       "      <td>0.106509</td>\n",
       "      <td>0.018795</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.134122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.253938e+18</td>\n",
       "      <td>5.092492e+17</td>\n",
       "      <td>1.253147e+18</td>\n",
       "      <td>0.587104</td>\n",
       "      <td>7.983032</td>\n",
       "      <td>2.361425</td>\n",
       "      <td>0.271795</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.187783</td>\n",
       "      <td>26.228507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262140</td>\n",
       "      <td>1.995957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>87.115385</td>\n",
       "      <td>0.107851</td>\n",
       "      <td>0.019189</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.931561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.253943e+18</td>\n",
       "      <td>4.373013e+17</td>\n",
       "      <td>1.252977e+18</td>\n",
       "      <td>0.851322</td>\n",
       "      <td>16.968023</td>\n",
       "      <td>3.998599</td>\n",
       "      <td>0.268508</td>\n",
       "      <td>0.263674</td>\n",
       "      <td>0.147034</td>\n",
       "      <td>25.194177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263564</td>\n",
       "      <td>1.621993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>86.773359</td>\n",
       "      <td>0.092090</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.817944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.253964e+18</td>\n",
       "      <td>2.590995e+17</td>\n",
       "      <td>1.253206e+18</td>\n",
       "      <td>1.709344</td>\n",
       "      <td>23.609235</td>\n",
       "      <td>5.484609</td>\n",
       "      <td>0.284257</td>\n",
       "      <td>0.422501</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>25.865568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267071</td>\n",
       "      <td>11.895564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>105.836012</td>\n",
       "      <td>0.033074</td>\n",
       "      <td>0.019445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.803187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1.253928e+18</td>\n",
       "      <td>2.153111e+17</td>\n",
       "      <td>1.253079e+18</td>\n",
       "      <td>6.722018</td>\n",
       "      <td>114.163952</td>\n",
       "      <td>27.337888</td>\n",
       "      <td>0.291443</td>\n",
       "      <td>0.291645</td>\n",
       "      <td>0.177614</td>\n",
       "      <td>25.036784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276124</td>\n",
       "      <td>723.734861</td>\n",
       "      <td>0.459275</td>\n",
       "      <td>0.304256</td>\n",
       "      <td>0.58907</td>\n",
       "      <td>71.158697</td>\n",
       "      <td>0.090221</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>3.658434</td>\n",
       "      <td>148.223857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id     author_id  associated_tweet  reply_count  \\\n",
       "user_group_db                                                              \n",
       " 1             1.253942e+18  4.277929e+17      1.253449e+18     0.380268   \n",
       " 2             1.253957e+18  5.748085e+17      1.253366e+18     0.904011   \n",
       " 4             1.253938e+18  5.092492e+17      1.253147e+18     0.587104   \n",
       " 0             1.253943e+18  4.373013e+17      1.252977e+18     0.851322   \n",
       " 3             1.253964e+18  2.590995e+17      1.253206e+18     1.709344   \n",
       "-1             1.253928e+18  2.153111e+17      1.253079e+18     6.722018   \n",
       "\n",
       "               favorite_count  retweet_count  not_english  hashtag_count  \\\n",
       "user_group_db                                                              \n",
       " 1                   2.052272       0.273605     0.260551       0.175653   \n",
       " 2                   5.859303       1.370809     0.278667       0.402367   \n",
       " 4                   7.983032       2.361425     0.271795       0.326923   \n",
       " 0                  16.968023       3.998599     0.268508       0.263674   \n",
       " 3                  23.609235       5.484609     0.284257       0.422501   \n",
       "-1                 114.163952      27.337888     0.291443       0.291645   \n",
       "\n",
       "               mention_count  word_count  ...  big_feelings       ratio  \\\n",
       "user_group_db                             ...                             \n",
       " 1                  0.145750   25.222745  ...      0.262100    1.127193   \n",
       " 2                  0.202498   26.662722  ...      0.267239    2.813750   \n",
       " 4                  0.187783   26.228507  ...      0.262140    1.995957   \n",
       " 0                  0.147034   25.194177  ...      0.263564    1.621993   \n",
       " 3                  0.177200   25.865568  ...      0.267071   11.895564   \n",
       "-1                  0.177614   25.036784  ...      0.276124  723.734861   \n",
       "\n",
       "                has_url  has_location  has_bio     len_bio  ratio_num_user  \\\n",
       "user_group_db                                                                \n",
       " 1             0.000000      0.000000  0.00000    3.000000        0.139479   \n",
       " 2             1.000000      1.000000  1.00000   48.174227        0.106509   \n",
       " 4             0.000000      1.000000  1.00000   87.115385        0.107851   \n",
       " 0             0.000000      0.000000  1.00000   86.773359        0.092090   \n",
       " 3             1.000000      0.000000  1.00000  105.836012        0.033074   \n",
       "-1             0.459275      0.304256  0.58907   71.158697        0.090221   \n",
       "\n",
       "               emotional_range  user_group      target  \n",
       "user_group_db                                           \n",
       " 1                    0.019135    2.000000    2.706146  \n",
       " 2                    0.018795    4.000000    8.134122  \n",
       " 4                    0.019189    4.000000   10.931561  \n",
       " 0                    0.019169    0.000000   21.817944  \n",
       " 3                    0.019445    1.000000   30.803187  \n",
       "-1                    0.018640    3.658434  148.223857  \n",
       "\n",
       "[6 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.groupby(['user_group_db']).mean().sort_values(['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_outlier = tweet[tweet['user_group_db'] == -1]\n",
    "db_0 = tweet[tweet['user_group_db'] == 0]\n",
    "db_1 = tweet[tweet['user_group_db'] == 1]\n",
    "db_2 = tweet[tweet['user_group_db'] == 2]\n",
    "db_3 = tweet[tweet['user_group_db'] == 3]\n",
    "db_4 = tweet[tweet['user_group_db'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_trends(df, column, ngram_min, ngram_max, top_word_count):\n",
    "    cvec = CountVectorizer(stop_words = 'english', min_df=1, max_df=0.25, ngram_range=(ngram_min, ngram_max))\n",
    "    term_mat = cvec.fit_transform(df[column])\n",
    "    print(f'Number of unique items: {len(cvec.get_feature_names())}')\n",
    "    print()\n",
    "    term_df = pd.DataFrame(term_mat.toarray(), columns=cvec.get_feature_names())\n",
    "    top_words = pd.DataFrame(term_df.mean().sort_values(ascending = False).head(top_word_count), columns = ['Frequency'])\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing Phrases for Entire Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 39641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_one = top_trends(tweet, 'clean_text', 1, 1, 20)\n",
    "tweet_one.to_csv('../data/tweet_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 255173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_two = top_trends(tweet, 'clean_text', 2, 2, 20)\n",
    "tweet_two.to_csv('../data/tweet_two.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 288481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_three = top_trends(tweet, 'clean_text', 3, 3, 20)\n",
    "tweet_three.to_csv('../data/tweet_three.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 265656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_four = top_trends(tweet, 'clean_text', 4, 4, 20)\n",
    "tweet_four.to_csv('../data/tweet_four.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases for Outlier Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 7063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outlier_one = top_trends(db_outlier, 'clean_text', 1, 1, 20)\n",
    "outlier_one.to_csv('../data/outlier_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 17765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outlier_two = top_trends(db_outlier, 'clean_text', 2, 2, 20)\n",
    "outlier_two.to_csv('../data/outlier_two.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 17210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outlier_three = top_trends(db_outlier, 'clean_text', 3, 3, 20)\n",
    "outlier_three.to_csv('../data/outlier_three.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 15627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outlier_four = top_trends(db_outlier, 'clean_text', 4, 4, 20)\n",
    "outlier_four.to_csv('../data/outlier_four.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases for our main priority to focus in on when identifying fake news tweets (group 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 18409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concern_one = top_trends(db_3, 'clean_text', 1, 1, 20)\n",
    "concern_one.to_csv('../data/concern_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 68603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concern_two = top_trends(db_3, 'clean_text', 2, 2, 20)\n",
    "concern_two.to_csv('../data/concern_two.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 68803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concern_three = top_trends(db_3, 'clean_text', 3, 3, 20)\n",
    "concern_three.to_csv('../data/concern_three.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 62680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concern_four = top_trends(db_3, 'clean_text', 4, 4, 20)\n",
    "concern_four.to_csv('../data/concern_four.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>associated_tweet</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>not_english</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>...</th>\n",
       "      <th>big_feelings</th>\n",
       "      <th>ratio</th>\n",
       "      <th>has_url</th>\n",
       "      <th>has_location</th>\n",
       "      <th>has_bio</th>\n",
       "      <th>len_bio</th>\n",
       "      <th>ratio_num_user</th>\n",
       "      <th>emotional_range</th>\n",
       "      <th>user_group</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_group_db</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.253942e+18</td>\n",
       "      <td>4.277929e+17</td>\n",
       "      <td>1.253449e+18</td>\n",
       "      <td>0.380268</td>\n",
       "      <td>2.052272</td>\n",
       "      <td>0.273605</td>\n",
       "      <td>0.260551</td>\n",
       "      <td>0.175653</td>\n",
       "      <td>0.145750</td>\n",
       "      <td>25.222745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>1.127193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.139479</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.706146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.253957e+18</td>\n",
       "      <td>5.748085e+17</td>\n",
       "      <td>1.253366e+18</td>\n",
       "      <td>0.904011</td>\n",
       "      <td>5.859303</td>\n",
       "      <td>1.370809</td>\n",
       "      <td>0.278667</td>\n",
       "      <td>0.402367</td>\n",
       "      <td>0.202498</td>\n",
       "      <td>26.662722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267239</td>\n",
       "      <td>2.813750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>48.174227</td>\n",
       "      <td>0.106509</td>\n",
       "      <td>0.018795</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.134122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.253938e+18</td>\n",
       "      <td>5.092492e+17</td>\n",
       "      <td>1.253147e+18</td>\n",
       "      <td>0.587104</td>\n",
       "      <td>7.983032</td>\n",
       "      <td>2.361425</td>\n",
       "      <td>0.271795</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.187783</td>\n",
       "      <td>26.228507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262140</td>\n",
       "      <td>1.995957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>87.115385</td>\n",
       "      <td>0.107851</td>\n",
       "      <td>0.019189</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.931561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.253943e+18</td>\n",
       "      <td>4.373013e+17</td>\n",
       "      <td>1.252977e+18</td>\n",
       "      <td>0.851322</td>\n",
       "      <td>16.968023</td>\n",
       "      <td>3.998599</td>\n",
       "      <td>0.268508</td>\n",
       "      <td>0.263674</td>\n",
       "      <td>0.147034</td>\n",
       "      <td>25.194177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263564</td>\n",
       "      <td>1.621993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>86.773359</td>\n",
       "      <td>0.092090</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.817944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.253964e+18</td>\n",
       "      <td>2.590995e+17</td>\n",
       "      <td>1.253206e+18</td>\n",
       "      <td>1.709344</td>\n",
       "      <td>23.609235</td>\n",
       "      <td>5.484609</td>\n",
       "      <td>0.284257</td>\n",
       "      <td>0.422501</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>25.865568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267071</td>\n",
       "      <td>11.895564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>105.836012</td>\n",
       "      <td>0.033074</td>\n",
       "      <td>0.019445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.803187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>1.253928e+18</td>\n",
       "      <td>2.153111e+17</td>\n",
       "      <td>1.253079e+18</td>\n",
       "      <td>6.722018</td>\n",
       "      <td>114.163952</td>\n",
       "      <td>27.337888</td>\n",
       "      <td>0.291443</td>\n",
       "      <td>0.291645</td>\n",
       "      <td>0.177614</td>\n",
       "      <td>25.036784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276124</td>\n",
       "      <td>723.734861</td>\n",
       "      <td>0.459275</td>\n",
       "      <td>0.304256</td>\n",
       "      <td>0.58907</td>\n",
       "      <td>71.158697</td>\n",
       "      <td>0.090221</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>3.658434</td>\n",
       "      <td>148.223857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id     author_id  associated_tweet  reply_count  \\\n",
       "user_group_db                                                              \n",
       " 1             1.253942e+18  4.277929e+17      1.253449e+18     0.380268   \n",
       " 2             1.253957e+18  5.748085e+17      1.253366e+18     0.904011   \n",
       " 4             1.253938e+18  5.092492e+17      1.253147e+18     0.587104   \n",
       " 0             1.253943e+18  4.373013e+17      1.252977e+18     0.851322   \n",
       " 3             1.253964e+18  2.590995e+17      1.253206e+18     1.709344   \n",
       "-1             1.253928e+18  2.153111e+17      1.253079e+18     6.722018   \n",
       "\n",
       "               favorite_count  retweet_count  not_english  hashtag_count  \\\n",
       "user_group_db                                                              \n",
       " 1                   2.052272       0.273605     0.260551       0.175653   \n",
       " 2                   5.859303       1.370809     0.278667       0.402367   \n",
       " 4                   7.983032       2.361425     0.271795       0.326923   \n",
       " 0                  16.968023       3.998599     0.268508       0.263674   \n",
       " 3                  23.609235       5.484609     0.284257       0.422501   \n",
       "-1                 114.163952      27.337888     0.291443       0.291645   \n",
       "\n",
       "               mention_count  word_count  ...  big_feelings       ratio  \\\n",
       "user_group_db                             ...                             \n",
       " 1                  0.145750   25.222745  ...      0.262100    1.127193   \n",
       " 2                  0.202498   26.662722  ...      0.267239    2.813750   \n",
       " 4                  0.187783   26.228507  ...      0.262140    1.995957   \n",
       " 0                  0.147034   25.194177  ...      0.263564    1.621993   \n",
       " 3                  0.177200   25.865568  ...      0.267071   11.895564   \n",
       "-1                  0.177614   25.036784  ...      0.276124  723.734861   \n",
       "\n",
       "                has_url  has_location  has_bio     len_bio  ratio_num_user  \\\n",
       "user_group_db                                                                \n",
       " 1             0.000000      0.000000  0.00000    3.000000        0.139479   \n",
       " 2             1.000000      1.000000  1.00000   48.174227        0.106509   \n",
       " 4             0.000000      1.000000  1.00000   87.115385        0.107851   \n",
       " 0             0.000000      0.000000  1.00000   86.773359        0.092090   \n",
       " 3             1.000000      0.000000  1.00000  105.836012        0.033074   \n",
       "-1             0.459275      0.304256  0.58907   71.158697        0.090221   \n",
       "\n",
       "               emotional_range  user_group      target  \n",
       "user_group_db                                           \n",
       " 1                    0.019135    2.000000    2.706146  \n",
       " 2                    0.018795    4.000000    8.134122  \n",
       " 4                    0.019189    4.000000   10.931561  \n",
       " 0                    0.019169    0.000000   21.817944  \n",
       " 3                    0.019445    1.000000   30.803187  \n",
       "-1                    0.018640    3.658434  148.223857  \n",
       "\n",
       "[6 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.groupby(['user_group_db']).mean().sort_values(['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd priority concern (group 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 24268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priority_2_one = top_trends(db_0, 'clean_text', 1, 1, 5)\n",
    "priority_2_one.to_csv('../data/priority_2_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 129028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priority_2_four = top_trends(db_0, 'clean_text', 4, 4, 5)\n",
    "priority_2_four.to_csv('../data/priority_2_four.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd priority concern (group 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 6455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priority_3_one = top_trends(db_4, 'clean_text', 1, 1, 5)\n",
    "priority_3_one.to_csv('../data/priority_3_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 14883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priority_3_four = top_trends(db_4, 'clean_text', 4, 4, 5)\n",
    "priority_3_four.to_csv('../data/priority_3_four.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th priority concern (group 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 6511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priority_4_one = top_trends(db_2, 'clean_text', 1, 1, 5)\n",
    "priority_4_one.to_csv('../data/priority_4_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 13706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priority_4_four = top_trends(db_2, 'clean_text', 4, 4, 5)\n",
    "priority_4_four.to_csv('../data/priority_4_four.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5th priority concern (group 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 9799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priority_5_one = top_trends(db_1, 'clean_text', 1, 1, 5)\n",
    "priority_5_one.to_csv('../data/priority_5_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 33284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "priority_5_four = top_trends(db_1, 'clean_text', 4, 4, 5)\n",
    "priority_5_four.to_csv('../data/priority_5_four.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round em up!\n",
    "Putting all the phrases for each group back into one DataFrames for each phrase length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_one = pd.read_csv('../data/outlier_one.csv')\n",
    "outlier_four = pd.read_csv('../data/outlier_four.csv')\n",
    "concern_one = pd.read_csv('../data/concern_one.csv')\n",
    "concern_four = pd.read_csv('../data/concern_four.csv')\n",
    "priority_2_one = pd.read_csv('../data/priority_2_one.csv')\n",
    "priority_2_four = pd.read_csv('../data/priority_2_four.csv')\n",
    "priority_3_one = pd.read_csv('../data/priority_3_one.csv')\n",
    "priority_3_four = pd.read_csv('../data/priority_3_four.csv')\n",
    "priority_4_one = pd.read_csv('../data/priority_4_one.csv')\n",
    "priority_4_four = pd.read_csv('../data/priority_4_four.csv')\n",
    "priority_5_one = pd.read_csv('../data/priority_5_one.csv')\n",
    "priority_5_four = pd.read_csv('../data/priority_5_four.csv')\n",
    "\n",
    "all_tweets_one = pd.read_csv('../data/tweet_one.csv')\n",
    "all_tweets_four = pd.read_csv('../data/tweet_four.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_one = outlier_one.head()\n",
    "outlier_four = outlier_four.head()\n",
    "concern_one = concern_one.head()\n",
    "concern_four = concern_four.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_one = outlier_one.rename(columns = { 'Unnamed: 0' : 'Outlier'})        \n",
    "concern_one = concern_one.rename(columns = { 'Unnamed: 0' : '1st Priority'})        \n",
    "priority_2_one = priority_2_one.rename(columns = { 'Unnamed: 0' : '2nd Priority'}) \n",
    "priority_3_one = priority_3_one.rename(columns = { 'Unnamed: 0' : '3rd Priority'}) \n",
    "priority_4_one = priority_4_one.rename(columns = { 'Unnamed: 0' : '4th Priority'}) \n",
    "priority_5_one = priority_5_one.rename(columns = { 'Unnamed: 0' : '5th Priority'}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_four = outlier_four.rename(columns = { 'Unnamed: 0' : 'Outlier'})             \n",
    "concern_four = concern_four.rename(columns = { 'Unnamed: 0' : '1st Priority'})            \n",
    "priority_2_four = priority_2_four.rename(columns = { 'Unnamed: 0' : '2nd Priority'})\n",
    "priority_3_four = priority_3_four.rename(columns = { 'Unnamed: 0' : '3rd Priority'}) \n",
    "priority_4_four = priority_4_four.rename(columns = { 'Unnamed: 0' : '4th Priority'})\n",
    "priority_5_four = priority_5_four.rename(columns = { 'Unnamed: 0' : '5th Priority'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlier</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trump</td>\n",
       "      <td>0.207042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "      <td>0.140305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just</td>\n",
       "      <td>0.071466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>0.069364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virus</td>\n",
       "      <td>0.059380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Outlier  Frequency\n",
       "0   trump   0.207042\n",
       "1  people   0.140305\n",
       "2    just   0.071466\n",
       "3    like   0.069364\n",
       "4   virus   0.059380"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_one_word = outlier_one[['Outlier']]\n",
    "top_one_word['1st Priority'] = concern_one['1st Priority']\n",
    "top_one_word['2nd Priority'] = priority_2_one['2nd Priority']\n",
    "top_one_word['3rd Priority'] = priority_3_one['3rd Priority']\n",
    "top_one_word['4th Priority'] = priority_4_one['4th Priority']\n",
    "top_one_word['5th Priority'] = priority_5_one['5th Priority']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_one_word.to_csv('../data/clusters_top_one_word.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlier</th>\n",
       "      <th>1st Priority</th>\n",
       "      <th>2nd Priority</th>\n",
       "      <th>3rd Priority</th>\n",
       "      <th>4th Priority</th>\n",
       "      <th>5th Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trump</td>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "      <td>trump</td>\n",
       "      <td>trump</td>\n",
       "      <td>trump</td>\n",
       "      <td>trump</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just</td>\n",
       "      <td>just</td>\n",
       "      <td>just</td>\n",
       "      <td>just</td>\n",
       "      <td>just</td>\n",
       "      <td>just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virus</td>\n",
       "      <td>time</td>\n",
       "      <td>said</td>\n",
       "      <td>drink</td>\n",
       "      <td>virus</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Outlier 1st Priority 2nd Priority 3rd Priority 4th Priority 5th Priority\n",
       "0   trump       people       people       people       people       people\n",
       "1  people        trump        trump        trump        trump        trump\n",
       "2    just         just         just         just         just         just\n",
       "3    like         like         like         like         like         said\n",
       "4   virus         time         said        drink        virus         like"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_one_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the top words for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_four_word = outlier_four[['Outlier']]\n",
    "top_four_word['1st Priority'] = concern_four['1st Priority']\n",
    "top_four_word['2nd Priority'] = priority_2_four['2nd Priority']\n",
    "top_four_word['3rd Priority'] = priority_3_four['3rd Priority']\n",
    "top_four_word['4th Priority'] = priority_4_four['4th Priority']\n",
    "top_four_word['5th Priority'] = priority_5_four['5th Priority']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_four_word.to_csv('../data/clusters_top_four_word.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlier</th>\n",
       "      <th>1st Priority</th>\n",
       "      <th>2nd Priority</th>\n",
       "      <th>3rd Priority</th>\n",
       "      <th>4th Priority</th>\n",
       "      <th>5th Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stop panic end total</td>\n",
       "      <td>stop panic end total</td>\n",
       "      <td>like injection inside cleaning</td>\n",
       "      <td>way like injection inside</td>\n",
       "      <td>group peddling cure wrote</td>\n",
       "      <td>stop panic end total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data stop panic end</td>\n",
       "      <td>data stop panic end</td>\n",
       "      <td>way like injection inside</td>\n",
       "      <td>like injection inside cleaning</td>\n",
       "      <td>people actually exactly caution</td>\n",
       "      <td>way like injection inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cure wrote trump week</td>\n",
       "      <td>cure wrote trump week</td>\n",
       "      <td>minute way like injection</td>\n",
       "      <td>minute way like injection</td>\n",
       "      <td>meant people actually exactly</td>\n",
       "      <td>data stop panic end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>group peddling cure wrote</td>\n",
       "      <td>group peddling cure wrote</td>\n",
       "      <td>stop panic end total</td>\n",
       "      <td>minute minute way like</td>\n",
       "      <td>leader group peddling cure</td>\n",
       "      <td>like injection inside cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>leader group peddling cure</td>\n",
       "      <td>need personal mug check</td>\n",
       "      <td>data stop panic end</td>\n",
       "      <td>knocks minute minute way</td>\n",
       "      <td>hot cups hot coffee</td>\n",
       "      <td>minute way like injection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Outlier               1st Priority  \\\n",
       "0        stop panic end total       stop panic end total   \n",
       "1         data stop panic end        data stop panic end   \n",
       "2       cure wrote trump week      cure wrote trump week   \n",
       "3   group peddling cure wrote  group peddling cure wrote   \n",
       "4  leader group peddling cure    need personal mug check   \n",
       "\n",
       "                     2nd Priority                    3rd Priority  \\\n",
       "0  like injection inside cleaning       way like injection inside   \n",
       "1       way like injection inside  like injection inside cleaning   \n",
       "2       minute way like injection       minute way like injection   \n",
       "3            stop panic end total          minute minute way like   \n",
       "4             data stop panic end        knocks minute minute way   \n",
       "\n",
       "                      4th Priority                    5th Priority  \n",
       "0        group peddling cure wrote            stop panic end total  \n",
       "1  people actually exactly caution       way like injection inside  \n",
       "2    meant people actually exactly             data stop panic end  \n",
       "3       leader group peddling cure  like injection inside cleaning  \n",
       "4              hot cups hot coffee       minute way like injection  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_four_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top 4 word phrases for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_one = pd.read_csv('../data/tweet_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_one = all_tweets_one.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_four = all_tweets_four.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_one = all_tweets_one.rename(columns = { 'Unnamed: 0' : 'Top Words'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_four = all_tweets_four.rename(columns = { 'Unnamed: 0' : 'Top Groups of Words'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tweet_stats = all_tweets_one[['Top Words']]\n",
    "top_tweet_stats['Top Groups of Words'] = all_tweets_four['Top Groups of Words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Words</th>\n",
       "      <th>Top Groups of Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people</td>\n",
       "      <td>stop panic end total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trump</td>\n",
       "      <td>data stop panic end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just</td>\n",
       "      <td>way like injection inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>like injection inside cleaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>said</td>\n",
       "      <td>minute way like injection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>drink</td>\n",
       "      <td>knocks minute minute way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>know</td>\n",
       "      <td>minute minute way like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>virus</td>\n",
       "      <td>cure wrote trump week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>president</td>\n",
       "      <td>peddling cure wrote trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>think</td>\n",
       "      <td>group peddling cure wrote</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Top Words             Top Groups of Words\n",
       "0     people            stop panic end total\n",
       "1      trump             data stop panic end\n",
       "2       just       way like injection inside\n",
       "3       like  like injection inside cleaning\n",
       "4       said       minute way like injection\n",
       "5      drink        knocks minute minute way\n",
       "6       know          minute minute way like\n",
       "7      virus           cure wrote trump week\n",
       "8  president       peddling cure wrote trump\n",
       "9      think       group peddling cure wrote"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tweet_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the top overall words and phrases for all tweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
